{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Script for retrieving and transforming FT content\n",
    "\n",
    "In the first part we get the content data using S-API and C-API, then we transform the content in order to use it as a training set for the Tensorflow/Textsum model.\n",
    "\n",
    "Finally we experiment with Sumy and Gensim which can be used to produce (extractive) summaries. Unuspervised methods like those can be used to build the training set for our DNN model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "import os, sys\n",
    "from IPython.display import display\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Define the absolute path of directory where the data should be stored \n",
    "data_path = \"/home/gkastro/title-prediction-tensorflow/content-data/\"\n",
    "\n",
    "# Define the starting year for our search\n",
    "min_year = 2008\n",
    "\n",
    "# Create the directories/folders for the data to be stored\n",
    "if not os.path.isdir(data_path+\"/text-data\"):\n",
    "    os.mkdir(data_path+\"/text-data\")\n",
    "if not os.path.isdir(data_path+\"/vocabs\"):\n",
    "    os.mkdir(data_path+\"/vocabs\")\n",
    "for year in range(min_year, 2018):\n",
    "    if not os.path.isdir(data_path+\"/\"+str(year)):\n",
    "        os.mkdir(data_path+\"/\"+str(year))\n",
    "    for month in range(1,13):\n",
    "        if not os.path.isdir(data_path+\"/\"+str(year)+\"/\"+str(month)):\n",
    "            os.mkdir(data_path+\"/\"+str(year)+\"/\"+str(month))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Getting article data through S-API and C-API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# The S-API and the C-API keys should be stored in environment variables SAPI_key and CAPI_key\n",
    "error_dir = data_path+\"errors/\"\n",
    "start = time.time()\n",
    "\n",
    "# We need to define the date after which we begin our search.\n",
    "# Naming this variable min_date might seem more appropriate but the primary use of it is to keep track of the\n",
    "# most recent date that has been fetched, while we perform requests one after the other.\n",
    "max_date = str(min_year)+\"-01-01T00:00:00Z\"\n",
    "\n",
    "# Define the number of iterations/requests, 100 results are brought back from each request,\n",
    "# out of which some articles might not be available through C-API.\n",
    "# So after performing 1000 requests we should expect to have retrieved ~95,000 articles\n",
    "if \"SAPI_key\" in os.environ and \"CAPI_key\"in os.environ:\n",
    "    s_api_key = os.environ[\"SAPI_key\"]\n",
    "    c_api_key = os.environ[\"CAPI_key\"]\n",
    "    for y in range(0,1000):\n",
    "        headers = {'Content-Type': 'application/json'}\n",
    "        payload = {\"queryString\":\"lastPublishDateTime:>\"+max_date,\n",
    "                   \"queryContext\":{\n",
    "                       \"curations\":[\"ARTICLES\", \"BLOGS\"]\n",
    "                   },\n",
    "                   \"resultContext\":{\n",
    "                       \"maxResults\":100, \n",
    "                       \"offset\":0,\n",
    "                       \"aspects\":[\"title\", \"metadata\", \"lifecycle\"],\n",
    "                       \"sortOrder\":\"ASC\",\n",
    "                       \"sortField\":\"lastPublishDateTime\"\n",
    "                   } \n",
    "                  }\n",
    "        r1 = requests.post(\"https://api.ft.com/content/search/v1?apiKey=\"+str(s_api_key), headers=headers, json=payload)\n",
    "        # If any error occurs while performing a request we carry on with the next request\n",
    "        if r1.status_code >= 400:\n",
    "            continue\n",
    "        response_json1 = r1.json()\n",
    "        # If there is no article matching our search then we break our request-loop,\n",
    "        # since we have reached the present day or no more article are available\n",
    "        if response_json1[\"results\"][0][\"indexCount\"] == 0:\n",
    "            break\n",
    "        response_json1_length = len(response_json1[\"results\"][0][\"results\"])\n",
    "        # Update max_date to the publish date of most recent article fetched\n",
    "        max_date = response_json1[\"results\"][0][\"results\"][response_json1_length-1][\"lifecycle\"][\"lastPublishDateTime\"]   \n",
    "        # Iterate through the results of S-API in order to get data through the enriched content API\n",
    "        for i in response_json1[\"results\"][0][\"results\"]:\n",
    "            if \"title\" in i.keys() and \"id\" in i.keys():\n",
    "                item_id = i[\"id\"]\n",
    "                tmp = i            \n",
    "                url = \"https://api.ft.com/enrichedcontent/\"+str(item_id)+\"?apiKey=\"+str(c_api_key)\n",
    "                r2 = requests.get(url)\n",
    "                if r2.status_code >= 400:\n",
    "                    continue\n",
    "                response_json2 = r2.json()\n",
    "                if \"errors\" in response_json2.keys():\n",
    "                    t = open(error_dir+item_id+\".json\", \"w\")\n",
    "                    json.dump({\"status_code\":r2.status_code, \"url\":r2.url, \"text\":r2.text}, t, indent=4)\n",
    "                    t.close()\n",
    "                    continue\n",
    "                if \"bodyXML\" in response_json2.keys():\n",
    "                    tmp[\"body\"] = response_json2[\"bodyXML\"]\n",
    "                    if \"prefLabel\" in response_json2.keys():\n",
    "                        tmp[\"prefLabel\"] = response_json2[\"prefLabel\"]\n",
    "                    else:\n",
    "                        tmp[\"prefLabel\"] = \"\"\n",
    "                    if \"standfirst\" in response_json2.keys():\n",
    "                        tmp[\"standfirst\"] = response_json2[\"standfirst\"]\n",
    "                    else:\n",
    "                        tmp[\"standfirst\"] = \"\"\n",
    "                    dtm = datetime.datetime.strptime(i[\"lifecycle\"][\"lastPublishDateTime\"], \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "                    # Saving all the data retrieved for each article in a separate json file, within a year and month folder\n",
    "                    f = open(data_path+str(dtm.year)+\"/\"+str(dtm.month)+\"/\"+item_id+\".json\", \"w\")\n",
    "                    json.dump(tmp, f, indent=4)\n",
    "                    f.close()\n",
    "                else:\n",
    "                    continue\n",
    "            else:\n",
    "                continue\n",
    "else:\n",
    "    print(\"API keys missing !\")\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Transforming the data and preparing the files for training Tensorflow NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "# Define the range of years and months of the articles that we want to transform\n",
    "for year in range(2008,2018):\n",
    "    for month in range(1,13):\n",
    "        # We will create a vocabulary and a content file for each year and month\n",
    "        content_file = data_path+\"text-data/text-data-\"+str(year)+\"-\"+str(month)\n",
    "        vocab_file = data_path+\"vocabs/vocab-\"+str(year)+\"-\"+str(month)+\".csv\"\n",
    "        file = open(content_file, \"w\")\n",
    "        vocab_df = pd.DataFrame(columns=[\"words\", \"frequency\"])\n",
    "        for filename in os.listdir(str(year)+\"/\"+str(month)+\"/\"):\n",
    "            if filename.endswith(\".json\"):\n",
    "                file2 = open(str(year)+\"/\"+str(month)+\"/\"+filename, \"r\")\n",
    "                content = json.load(file2)\n",
    "                file2.close()\n",
    "                title = content[\"title\"][\"title\"].replace(\"\\n\", \" \").replace(\"\\r\", \"\").replace(\"=\",\" \").replace(\"\\t\", \" \")\n",
    "                title_tok = unicodedata.normalize(\"NFKD\",title).encode(\"ascii\", \"ignore\")\n",
    "                body_raw = unicodedata.normalize(\"NFKD\",content[\"body\"]).encode(\"ascii\", \"ignore\")\n",
    "                # Getting rid of the html tags\n",
    "                soup = BeautifulSoup(body_raw, \"html.parser\")\n",
    "                soup_title = BeautifulSoup(title_tok, \"html.parser\")\n",
    "                # Tokenize sentences and add <s></s> tags\n",
    "                body_text = \" </s> <s> \".join(tokenizer.tokenize(soup.get_text())).replace(\"\\n\", \" \").replace(\"\\r\", \"\").replace(\"=\",\" \").replace(\"\\t\", \" \")\n",
    "                body = \"<d> <s> \"+body_text+\" </s> </d>\"\n",
    "                # Retrieve the tokens and create the vocabulary\n",
    "                tokens = nltk.wordpunct_tokenize(soup.text+soup_title.text)\n",
    "                words = [w for w in tokens]\n",
    "                words_freq = [words.count(w) for w in words]\n",
    "                d = {\"words\":words, \"frequency\":words_freq}\n",
    "                vocab_tmp = pd.DataFrame(data=d, columns=[\"words\", \"frequency\"])\n",
    "                vocab_tmp.drop_duplicates(keep=\"first\", inplace=True, subset=\"words\")\n",
    "                # If a vocabulary already exists for the given year and month then we update it\n",
    "                vocab_df = pd.merge(vocab_df, vocab_tmp, how = \"outer\", on = \"words\")\n",
    "                vocab_df.fillna(value=0, inplace=True)\n",
    "                vocab_df[\"frequency\"] = vocab_df.frequency_x + vocab_df.frequency_y\n",
    "                vocab_df.drop(labels=[\"frequency_x\", \"frequency_y\"], axis=1, inplace=True)\n",
    "                file.write(\"abstract=<d> <p> <s> \"+title+\" </s> </p> </d>\\tarticle= \"+body+\"\\n\")\n",
    "        file.close()\n",
    "        vocab_df.sort(ascending=False, columns=\"frequency\", inplace=True)\n",
    "        vocab_df.to_csv(data_path+\"vocabs/\"+vocab_file)\n",
    "        np.savetxt(data_path+\"vocabs/vocab-\"+str(year)+\"-\"+str(month)+\".txt\", vocab_df.values, fmt=\"%s %d\")\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# We repeat a very similar process in order to create a content file and a vocabulary for each year\n",
    "for year in range(2008, 2018):\n",
    "    vocab_df = pd.DataFrame(columns=[\"words\", \"frequency\"])\n",
    "    outfile = open(data_path+\"text-data/text-data-\"+str(year), \"w\")\n",
    "    for month in range(1, 13):\n",
    "        vocab_tmp = pd.read_csv(data_path+\"vocabs/vocab-\"+str(year)+\"-\"+str(month)+\".csv\", usecols=[\"words\", \"frequency\"])[[\"words\", \"frequency\"]]\n",
    "        vocab_df = pd.merge(vocab_df, vocab_tmp, how = \"outer\", on = \"words\")\n",
    "        vocab_df.fillna(value=0, inplace=True)\n",
    "        vocab_df[\"frequency\"] = vocab_df.frequency_x + vocab_df.frequency_y\n",
    "        vocab_df.drop(labels=[\"frequency_x\", \"frequency_y\"], axis=1, inplace=True)\n",
    "        infile = open(data_path+\"text-data/text-data-\"+str(year)+\"-\"+str(month))\n",
    "        for line in infile:\n",
    "            outfile.write(line)\n",
    "        infile.close()\n",
    "    vocab_df.sort(ascending=False, columns=\"frequency\", inplace=True)\n",
    "    vocab_df.to_csv(data_path+\"vocabs/vocab-\"+str(year)+\".csv\")\n",
    "    np.savetxt(data_path+\"vocabs/vocab-\"+str(year)+\".txt\", vocab_df.values, fmt=\"%s %d\")\n",
    "    outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Finally we iterate again over our data to get the content file and the vocabulary for all the articles\n",
    "outfile = open(data_path+\"text-data/text-data\", \"w\")\n",
    "vocab_df = pd.DataFrame(columns=[\"words\", \"frequency\"])\n",
    "for year in range(2008, 2018):\n",
    "    vocab_tmp = pd.read_csv(data_path+\"vocabs/vocab-\"+str(year)+\".csv\", usecols=[\"words\", \"frequency\"])[[\"words\", \"frequency\"]]\n",
    "    vocab_tmp = vocab_tmp.loc[vocab_tmp[\"words\"]!=\"0\"]\n",
    "    vocab_tmp.to_csv(data_path+\"vocabs/vocab-\"+str(year)+\".csv\")\n",
    "    vocab_df = pd.merge(vocab_df, vocab_tmp, how = \"outer\", on = \"words\")\n",
    "    vocab_df.fillna(value=0, inplace=True)\n",
    "    vocab_df[\"frequency\"] = vocab_df.frequency_x + vocab_df.frequency_y\n",
    "    vocab_df.drop(labels=[\"frequency_x\", \"frequency_y\"], axis=1, inplace=True)\n",
    "    infile = open(data_path+\"text-data/text-data-\"+str(year))\n",
    "    for line in infile:\n",
    "        outfile.write(line)\n",
    "    infile.close()\n",
    "# We need to add the following tokens in the vocab, their frequencies are made up but shouldn't affect the model\n",
    "tmp = pd.DataFrame(data={\"words\":[\"<s>\", \"</s>\", \"<PAD>\",\"<UNK>\"], \"frequency\":[6000000, 6000000, 3, 2000000]}, columns = [\"words\", \"frequency\"])\n",
    "vocab_df = vocab_df.append(tmp, ignore_index=True)\n",
    "vocab_df.sort(ascending=False, columns=\"frequency\", inplace=True)\n",
    "# Uncomment the following line in order to keep only the 300,000 most common tokens\n",
    "# vocab_df = vocab_df.iloc[0:300000,:]\n",
    "vocab_df.to_csv(data_path+\"vocabs/vocab.csv\")\n",
    "np.savetxt(data_path+\"vocabs/vocab\", vocab_df.values, fmt=\"%s %d\")\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Other Summarization tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Loading one of the articles\n",
    "filename = os.listdir(data_path+\"2016/1/\")[0]\n",
    "file = open(data_path+\"2016/1/\"+str(filename), \"r\")\n",
    "content = json.load(file)\n",
    "file.close()\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "body_raw = unicodedata.normalize(\"NFKD\",content[\"body\"]).encode(\"ascii\", \"ignore\")\n",
    "soup = BeautifulSoup(body_raw, \"html.parser\")\n",
    "# soup.text\n",
    "body_text = \"\\n\".join(tokenizer.tokenize(soup.get_text().replace(\"\\n\", \" \").replace(\"\\r\", \"\").replace(\"=\",\" \").replace(\"\\t\", \" \")))\n",
    "# body_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Gensim will calculate the most important sentences from the document, using TextRank and will return to us a summary\n",
    "# using only these sentences. It does not generate new sentences,\n",
    "# it just identifies and copies the most significant sentences.\n",
    "# More information at https://github.com/RaRe-Technologies/gensim\n",
    "from gensim.summarization import summarize, keywords\n",
    "\n",
    "# We can summarize the given article/document by defining the size of the summary \n",
    "# as a percentage of the size of the original one\n",
    "display(summarize(body_text, ratio=0.15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Or as an absolute word count\n",
    "display(summarize(body_text, word_count=50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# We can also extract the keywords from the document\n",
    "display(keywords(body_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Sumy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Sumy provides a variety of algorithms for text summarization, among which also TextRank.\n",
    "# More information about Sumy can be found at https://pypi.python.org/pypi/sumy\n",
    "\n",
    "from sumy.parsers.html import HtmlParser\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer\n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
    "from sumy.summarizers.text_rank import TextRankSummarizer\n",
    "from sumy.nlp.stemmers import Stemmer\n",
    "from sumy.utils import get_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Here we get summaries using TextRank, LexRank and LSA.\n",
    "# All three summaries produced are different, so it might be a good idea to use all three of them to find the \n",
    "# sentences on which they are overlapping, these sentences are more likely to contain valuable information.\n",
    "\n",
    "LANGUAGE = \"english\"\n",
    "SENTENCES_COUNT = 2\n",
    "parser = PlaintextParser.from_string(body_text, Tokenizer(LANGUAGE))\n",
    "# or for plain text files\n",
    "# parser = PlaintextParser.from_file(\"document.txt\", Tokenizer(LANGUAGE))\n",
    "stemmer = Stemmer(LANGUAGE)\n",
    "summarizers = [LexRankSummarizer, TextRankSummarizer, LsaSummarizer]\n",
    "for summarizer in summarizers:\n",
    "    if summarizers.index(summarizer) == 0:\n",
    "        name = \"LexRank\"\n",
    "    elif summarizers.index(summarizer) == 1:\n",
    "        name = \"TextRank\"\n",
    "    else:\n",
    "        name = \"LSA\"\n",
    "    print(\"Summarizing with: \"+name)\n",
    "    summarizer = summarizer(stemmer)\n",
    "    summarizer.stop_words = get_stop_words(LANGUAGE)\n",
    "    for sentence in summarizer(parser.document, SENTENCES_COUNT):\n",
    "        print(sentence)\n",
    "        # Printing the number of the sentence in the original article\n",
    "        print(body_text[0:body_text.index(str(sentence))].count(\"\\n\")+1)\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
