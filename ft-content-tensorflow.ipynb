{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "import os, sys\n",
    "from IPython.display import display\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the absolute path of directory where the data should be stored \n",
    "data_path = \"/home/gkastro/title-prediction-tensorflow/content-data/\"\n",
    "\n",
    "# Define the starting year for our search\n",
    "min_year = 2008\n",
    "\n",
    "# Create the directories/folders for the data to be stored\n",
    "if not os.path.isdir(data_path+\"/text-data\"):\n",
    "    os.mkdir(data_path+\"/text-data\")\n",
    "if not os.path.isdir(data_path+\"/vocabs\"):\n",
    "    os.mkdir(data_path+\"/vocabs\")\n",
    "for year in range(min_year, 2018):\n",
    "    if not os.path.isdir(data_path+\"/\"+str(year)):\n",
    "        os.mkdir(data_path+\"/\"+str(year))\n",
    "    for month in range(1,13):\n",
    "        if not os.path.isdir(data_path+\"/\"+str(year)+\"/\"+str(month)):\n",
    "            os.mkdir(data_path+\"/\"+str(year)+\"/\"+str(month))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Getting article data through S-API and C-API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# The S-API and the C-API keys should be stored in environment variables SAPI_key and CAPI_key\n",
    "error_dir = data_path+\"errors/\"\n",
    "start = time.time()\n",
    "\n",
    "# We need to define the date after which we begin our search.\n",
    "# Naming this variable min_date might seem more appropriate but the primary use of it is to keep track of the\n",
    "# most recent date that has been fetched, while we perform requests one after the other.\n",
    "max_date = str(min_year)+\"-01-01T00:00:00Z\"\n",
    "\n",
    "# Define the number of iterations/requests, 100 results are brought back from each request,\n",
    "# out of which some articles might not be available through C-API.\n",
    "# So after performing 1000 requests we should expect to have retrieved ~95,000 articles\n",
    "if \"SAPI_key\" in os.environ and \"CAPI_key\"in os.environ:\n",
    "    s_api_key = os.environ[\"SAPI_key\"]\n",
    "    c_api_key = os.environ[\"CAPI_key\"]\n",
    "    for y in range(0,1000):\n",
    "        headers = {'Content-Type': 'application/json'}\n",
    "        payload = {\"queryString\":\"lastPublishDateTime:>\"+max_date,\n",
    "                   \"queryContext\":{\n",
    "                       \"curations\":[\"ARTICLES\", \"BLOGS\"]\n",
    "                   },\n",
    "                   \"resultContext\":{\n",
    "                       \"maxResults\":100, \n",
    "                       \"offset\":0,\n",
    "                       \"aspects\":[\"title\", \"metadata\", \"lifecycle\"],\n",
    "                       \"sortOrder\":\"ASC\",\n",
    "                       \"sortField\":\"lastPublishDateTime\"\n",
    "                   } \n",
    "                  }\n",
    "        r1 = requests.post(\"https://api.ft.com/content/search/v1?apiKey=\"+str(s_api_key), headers=headers, json=payload)\n",
    "        # If any error occurs while performing a request we carry on with the next request\n",
    "        if r1.status_code >= 400:\n",
    "            continue\n",
    "        response_json1 = r1.json()\n",
    "        # If there is no article matching our search then we break our request-loop,\n",
    "        # since we have reached the present day or no more article are available\n",
    "        if response_json1[\"results\"][0][\"indexCount\"] == 0:\n",
    "            break\n",
    "        response_json1_length = len(response_json1[\"results\"][0][\"results\"])\n",
    "        # Update max_date to the publish date of most recent article fetched\n",
    "        max_date = response_json1[\"results\"][0][\"results\"][response_json1_length-1][\"lifecycle\"][\"lastPublishDateTime\"]   \n",
    "        # Iterate through the results of S-API in order to get data through the enriched content API\n",
    "        for i in response_json1[\"results\"][0][\"results\"]:\n",
    "            if \"title\" in i.keys() and \"id\" in i.keys():\n",
    "                item_id = i[\"id\"]\n",
    "                tmp = i            \n",
    "                url = \"https://api.ft.com/enrichedcontent/\"+str(item_id)+\"?apiKey=\"+str(c_api_key)\n",
    "                r2 = requests.get(url)\n",
    "                if r2.status_code >= 400:\n",
    "                    continue\n",
    "                response_json2 = r2.json()\n",
    "                if \"errors\" in response_json2.keys():\n",
    "                    t = open(error_dir+item_id+\".json\", \"w\")\n",
    "                    json.dump({\"status_code\":r2.status_code, \"url\":r2.url, \"text\":r2.text}, t, indent=4)\n",
    "                    t.close()\n",
    "                    continue\n",
    "                if \"bodyXML\" in response_json2.keys():\n",
    "                    tmp[\"body\"] = response_json2[\"bodyXML\"]\n",
    "                    if \"prefLabel\" in response_json2.keys():\n",
    "                        tmp[\"prefLabel\"] = response_json2[\"prefLabel\"]\n",
    "                    else:\n",
    "                        tmp[\"prefLabel\"] = \"\"\n",
    "                    if \"standfirst\" in response_json2.keys():\n",
    "                        tmp[\"standfirst\"] = response_json2[\"standfirst\"]\n",
    "                    else:\n",
    "                        tmp[\"standfirst\"] = \"\"\n",
    "                    dtm = datetime.datetime.strptime(i[\"lifecycle\"][\"lastPublishDateTime\"], \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "                    # Saving all the data retrieved for each article in a separate json file, within a year and month folder\n",
    "                    f = open(data_path+str(dtm.year)+\"/\"+str(dtm.month)+\"/\"+item_id+\".json\", \"w\")\n",
    "                    json.dump(tmp, f, indent=4)\n",
    "                    f.close()\n",
    "                else:\n",
    "                    continue\n",
    "            else:\n",
    "                continue\n",
    "else:\n",
    "    print(\"API keys missing !\")\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Transforming the data and preparing the files for training Tensorflow NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gkastro/.local/lib/python3.5/site-packages/ipykernel/__main__.py:38: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    },
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'latin-1' codec can't encode character '\\u2019' in position 0: ordinal not in range(256)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-83d627dcaf65>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mvocab_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"frequency\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mvocab_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavetxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"vocabs/vocab-\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myear\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"-\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"%s %d\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/gkastro/.local/lib/python3.5/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36msavetxt\u001b[0;34m(fname, X, fmt, delimiter, newline, header, footer, comments)\u001b[0m\n\u001b[1;32m   1213\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1214\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1215\u001b[0;31m                     \u001b[0mfh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masbytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1216\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1217\u001b[0m                     raise TypeError(\"Mismatch between array dtype ('%s') and \"\n",
      "\u001b[0;32m/home/gkastro/.local/lib/python3.5/site-packages/numpy/compat/py3k.py\u001b[0m in \u001b[0;36masbytes\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0masstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnicodeEncodeError\u001b[0m: 'latin-1' codec can't encode character '\\u2019' in position 0: ordinal not in range(256)"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "# Define the range of years and months of the articles that we want to transform\n",
    "for year in range(2008,2018):\n",
    "    for month in range(1,13):\n",
    "        # We will create a vocabulary and a content file for each year and month\n",
    "        content_file = \"text-data/text-data-\"+str(year)+\"-\"+str(month)\n",
    "        vocab_file = \"vocabs/vocab-\"+str(year)+\"-\"+str(month)+\".csv\"\n",
    "        file = open(content_file, \"w\")\n",
    "        vocab_df = pd.DataFrame(columns=[\"words\", \"frequency\"])\n",
    "        for filename in os.listdir(str(year)+\"/\"+str(month)+\"/\"):\n",
    "            if filename.endswith(\".json\"):\n",
    "                file2 = open(str(year)+\"/\"+str(month)+\"/\"+filename, \"r\")\n",
    "                content = json.load(file2)\n",
    "                file2.close()\n",
    "                title = content[\"title\"][\"title\"].replace(\"\\n\", \" \").replace(\"\\r\", \"\").replace(\"=\",\" \").replace(\"\\t\", \" \")\n",
    "                title_tok = unicodedata.normalize(\"NFKD\",title).encode(\"ascii\", \"ignore\")\n",
    "                body_raw = unicodedata.normalize(\"NFKD\",content[\"body\"]).encode(\"ascii\", \"ignore\")\n",
    "                # Getting rid of the html tags\n",
    "                soup = BeautifulSoup(body_raw, \"html.parser\")\n",
    "                soup_title = BeautifulSoup(title_tok, \"html.parser\")\n",
    "                # Tokenize sentences and add <s></s> tags\n",
    "                body_text = \" </s> <s> \".join(tokenizer.tokenize(soup.get_text())).replace(\"\\n\", \" \").replace(\"\\r\", \"\").replace(\"=\",\" \").replace(\"\\t\", \" \")\n",
    "                body = \"<d> <s> \"+body_text+\" </s> </d>\"\n",
    "                # Retrieve the tokens and create the vocabulary\n",
    "                tokens = nltk.wordpunct_tokenize(soup.text+soup_title.text)\n",
    "                words = [w.lower() for w in tokens]\n",
    "                words_freq = [words.count(w) for w in words]\n",
    "                d = {\"words\":words, \"frequency\":words_freq}\n",
    "                vocab_tmp = pd.DataFrame(data=d, columns=[\"words\", \"frequency\"])\n",
    "                vocab_tmp.drop_duplicates(keep=\"first\", inplace=True, subset=\"words\")\n",
    "                # If a vocabulary already exists for the given year and month then we update it\n",
    "                vocab_df = pd.merge(vocab_df, vocab_tmp, how = \"outer\", on = \"words\")\n",
    "                vocab_df.fillna(value=0, inplace=True)\n",
    "                vocab_df[\"frequency\"] = vocab_df.frequency_x + vocab_df.frequency_y\n",
    "                vocab_df.drop(labels=[\"frequency_x\", \"frequency_y\"], axis=1, inplace=True)\n",
    "                file.write(\"abstract=<d> <p> <s> \"+title+\" </s> </p> </d>\\tarticle= \"+body+\"\\n\")\n",
    "        file.close()\n",
    "        vocab_df.sort(ascending=False, columns=\"frequency\", inplace=True)\n",
    "        vocab_df.to_csv(data_path+\"vocabs/\"+vocab_file)\n",
    "        np.savetxt(data_path+\"vocabs/vocab-\"+str(year)+\"-\"+str(month)+\".txt\", vocab_df.values, fmt=\"%s %d\")\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# We repeat a very similar process in order to create a content file and a vocabulary for each year\n",
    "for year in range(2008, 2018):\n",
    "    vocab_df = pd.DataFrame(columns=[\"words\", \"frequency\"])\n",
    "    outfile = open(data_path+\"text-data/text-data-\"+str(year), \"w\")\n",
    "    for month in range(1, 13):\n",
    "        vocab_tmp = pd.read_csv(data_path+\"vocabs/vocab-\"+str(year)+\"-\"+str(month)+\".csv\", usecols=[\"words\", \"frequency\"])[[\"words\", \"frequency\"]]\n",
    "        vocab_df = pd.merge(vocab_df, vocab_tmp, how = \"outer\", on = \"words\")\n",
    "        vocab_df.fillna(value=0, inplace=True)\n",
    "        vocab_df[\"frequency\"] = vocab_df.frequency_x + vocab_df.frequency_y\n",
    "        vocab_df.drop(labels=[\"frequency_x\", \"frequency_y\"], axis=1, inplace=True)\n",
    "        infile = open(data_path+\"text-data/text-data-\"+str(year)+\"-\"+str(month))\n",
    "        for line in infile:\n",
    "            outfile.write(line)\n",
    "        infile.close()\n",
    "    vocab_df.sort(ascending=False, columns=\"frequency\", inplace=True)\n",
    "    vocab_df.to_csv(data_path+\"vocabs/vocab-\"+str(year)+\".csv\")\n",
    "    np.savetxt(data_path+\"vocabs/vocab-\"+str(year)+\".txt\", vocab_df.values, fmt=\"%s %d\")\n",
    "    outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Finally we iterate again over our data to get the content file and the vocabulary for all the articles\n",
    "outfile = open(data_path+\"text-data/text-data\", \"w\")\n",
    "vocab_df = pd.DataFrame(columns=[\"words\", \"frequency\"])\n",
    "for year in range(2008, 2018):\n",
    "    vocab_tmp = pd.read_csv(data_path+\"vocabs/vocab-\"+str(year)+\".csv\", usecols=[\"words\", \"frequency\"])[[\"words\", \"frequency\"]]\n",
    "    vocab_tmp = vocab_tmp.loc[vocab_tmp[\"words\"]!=\"0\"]\n",
    "    vocab_tmp.to_csv(data_path+\"vocabs/vocab-\"+str(year)+\".csv\")\n",
    "    vocab_df = pd.merge(vocab_df, vocab_tmp, how = \"outer\", on = \"words\")\n",
    "    vocab_df.fillna(value=0, inplace=True)\n",
    "    vocab_df[\"frequency\"] = vocab_df.frequency_x + vocab_df.frequency_y\n",
    "    vocab_df.drop(labels=[\"frequency_x\", \"frequency_y\"], axis=1, inplace=True)\n",
    "    infile = open(data_path+\"text-data/text-data-\"+str(year))\n",
    "    for line in infile:\n",
    "        outfile.write(line)\n",
    "    infile.close()\n",
    "# We need to add the following tokens in the vocab, their frequencies are made up but shouldn't affect the model\n",
    "tmp = pd.DataFrame(data={\"words\":[\"<s>\", \"</s>\", \"<PAD>\",\"<UNK>\"], \"frequency\":[6000000, 6000000, 3, 2000000]}, columns = [\"words\", \"frequency\"])\n",
    "vocab_df = vocab_df.append(tmp, ignore_index=True)\n",
    "vocab_df.sort(ascending=False, columns=\"frequency\", inplace=True)\n",
    "# Uncomment the following line in order to keep only the 300,000 most common tokens\n",
    "# vocab_df = vocab_df.iloc[0:300000,:]\n",
    "vocab_df.to_csv(data_path+\"vocabs/vocab.csv\")\n",
    "np.savetxt(data_path+\"vocabs/vocab\", vocab_df.values, fmt=\"%s %d\")\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Other Summarization tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Samsung Electronics has agreed to subject its factories to independent inspectors  a significant concession to activists who reiterated calls for it to accept blame for cancer among workers.\\nSamsung has spent more than a year trying to resolve longstanding complaints from dozens of former workers who say their exposure to toxic chemicals and radiation resulted in cancer and other serious diseases.\\nThe resultant scrutiny has threatened Samsungs brand at a time when it is trying to maintain investor confidence amid slowing smartphone sales and undergoing a leadership transition following the incapacitation of group patriarch Lee Kun-hee.\\nLast year, Samsung announced a Won100bn ($83m) fund to be used for compensation to ill former workers in what it called a goodwill gesture, as well as preventative measures.\\nWhile many former workers applied for compensation, the activist group that has led the campaign on the issue  including several former staff and their families  said that Samsung had failed to follow the recommendations of a mediation committee set up to find a compromise between the two sides.\\nSamsungs announcement that it will establish an independent ombudsman team, as recommended by the mediation committee, addresses a key demand of Sharps, the activist group.\\nIt had dismissed Samsungs previous proposal to deploy an inspection team appointed by the company, saying that such a teams independence would be suspect.\\nSamsung said it would faithfully implement recommendations from the ombudsman team, which will inspect its factories over the next three years.\\nAll sides agreed on the head of the team, a labour law professor, who will choose the other members himself.\\nHwang Sang-ki, the most prominent campaigner on the issue since his daughter died of leukaemia after working at a Samsung semiconductor plant, said he was withholding judgment on the deal.\\nWe will need to see whether Samsung will fully co-operate in opening their factories to the inspectors, he said.\\nSharps said it would continue its protests outside Samsungs Seoul headquarters and refuse to endorse the compensation payments until its remaining demands are met, including an admission of responsibility for the workers illnesses.\\nSouth Koreas courts have found a causal link between four cases of cancer or aplastic anaemia and exposure to dangerous substances in Samsung plants.\\nBut Samsung says there is no evidence of a link, noting that while studies have found elevated levels of chemicals and radiation in its plants, these were within legal limits.\\nSharps also wants Samsung to give control over the compensation to an independent body  another of the mediation committees suggestions  instead of dispensing it directly.\\nSamsung said that more than 150 people had applied for its programme, of whom more than 100 have accepted financial support, in return for which they waived their right to pursue legal action.\\nAlong with the financial aid, every recipient has received a heartfelt message of sympathy from Samsungs CEO, it said.\\nBut Mr Hwang said that the compensation to some applicants had fallen short of their medical expenses.\\nSamsung is just trying to settle the issue with money as quickly as possible, he said.\\nAdditional reporting by Kang Buseong'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading one of the articles\n",
    "filename = os.listdir(data_path+\"2016/1/\")[0]\n",
    "file = open(data_path+\"2016/1/\"+str(filename), \"r\")\n",
    "content = json.load(file)\n",
    "file.close()\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "body_raw = unicodedata.normalize(\"NFKD\",content[\"body\"]).encode(\"ascii\", \"ignore\")\n",
    "soup = BeautifulSoup(body_raw, \"html.parser\")\n",
    "# soup.text\n",
    "body_text = \"\\n\".join(tokenizer.tokenize(soup.get_text().replace(\"\\n\", \" \").replace(\"\\r\", \"\").replace(\"=\",\" \").replace(\"\\t\", \" \")))\n",
    "# body_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Samsung Electronics has agreed to subject its factories to independent inspectors  a significant concession to activists who reiterated calls for it to accept blame for cancer among workers.\\nWhile many former workers applied for compensation, the activist group that has led the campaign on the issue  including several former staff and their families  said that Samsung had failed to follow the recommendations of a mediation committee set up to find a compromise between the two sides.\\nSamsungs announcement that it will establish an independent ombudsman team, as recommended by the mediation committee, addresses a key demand of Sharps, the activist group.'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Gensim will calculate the most important sentences from the document, using TextRank and will return to us a summary\n",
    "# using only these sentences. It does not generate new sentences,\n",
    "# it just identifies and copies the most significant sentences.\n",
    "# More information at https://github.com/RaRe-Technologies/gensim\n",
    "from gensim.summarization import summarize, keywords\n",
    "\n",
    "# We can summarize the given article/document by defining the size of the summary \n",
    "# as a percentage of the size of the original one\n",
    "display(summarize(body_text, ratio=0.15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'While many former workers applied for compensation, the activist group that has led the campaign on the issue  including several former staff and their families  said that Samsung had failed to follow the recommendations of a mediation committee set up to find a compromise between the two sides.'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Or as an absolute word count\n",
    "display(summarize(body_text, word_count=50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samsung\n",
      "samsungs\n",
      "said\n",
      "committee\n",
      "independent\n",
      "teams independence\n",
      "team\n",
      "committees suggestions\n",
      "legal\n",
      "accepted financial\n",
      "demand\n",
      "demands\n",
      "link\n",
      "longstanding\n",
      "transition\n",
      "law\n",
      "implement\n",
      "including\n",
      "resulted\n",
      "resultant\n",
      "workers\n",
      "plant\n",
      "plants\n",
      "accept\n",
      "kun\n",
      "smartphone\n",
      "investor\n",
      "group patriarch\n",
      "lee\n"
     ]
    }
   ],
   "source": [
    "# We can also extract the keywords from the document\n",
    "display(keywords(body_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Sumy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Sumy provides a variety of algorithms for text summarization, among which also TextRank.\n",
    "# More information about Sumy can be found at https://pypi.python.org/pypi/sumy\n",
    "\n",
    "from sumy.parsers.html import HtmlParser\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer\n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
    "from sumy.summarizers.text_rank import TextRankSummarizer\n",
    "from sumy.nlp.stemmers import Stemmer\n",
    "from sumy.utils import get_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing with: LexRank\n",
      "Samsungs announcement that it will establish an independent ombudsman team, as recommended by the mediation committee, addresses a key demand of Sharps, the activist group.\n",
      "6\n",
      "Samsung said it would faithfully implement recommendations from the ombudsman team, which will inspect its factories over the next three years.\n",
      "8\n",
      "\n",
      "\n",
      "Summarizing with: TextRank\n",
      "While many former workers applied for compensation, the activist group that has led the campaign on the issue  including several former staff and their families  said that Samsung had failed to follow the recommendations of a mediation committee set up to find a compromise between the two sides.\n",
      "5\n",
      "Samsungs announcement that it will establish an independent ombudsman team, as recommended by the mediation committee, addresses a key demand of Sharps, the activist group.\n",
      "6\n",
      "\n",
      "\n",
      "Summarizing with: LSA\n",
      "The resultant scrutiny has threatened Samsungs brand at a time when it is trying to maintain investor confidence amid slowing smartphone sales and undergoing a leadership transition following the incapacitation of group patriarch Lee Kun-hee.\n",
      "3\n",
      "While many former workers applied for compensation, the activist group that has led the campaign on the issue  including several former staff and their families  said that Samsung had failed to follow the recommendations of a mediation committee set up to find a compromise between the two sides.\n",
      "5\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Here we get summaries using TextRank, LexRank and LSA.\n",
    "# All three summaries produced are different, so it might be a good idea to use all three of them to find the \n",
    "# sentences on which they are overlapping, these sentences are more likely to contain valuable information.\n",
    "\n",
    "LANGUAGE = \"english\"\n",
    "SENTENCES_COUNT = 2\n",
    "parser = PlaintextParser.from_string(body_text, Tokenizer(LANGUAGE))\n",
    "# or for plain text files\n",
    "# parser = PlaintextParser.from_file(\"document.txt\", Tokenizer(LANGUAGE))\n",
    "stemmer = Stemmer(LANGUAGE)\n",
    "summarizers = [LexRankSummarizer, TextRankSummarizer, LsaSummarizer]\n",
    "for summarizer in summarizers:\n",
    "    if summarizers.index(summarizer) == 0:\n",
    "        name = \"LexRank\"\n",
    "    elif summarizers.index(summarizer) == 1:\n",
    "        name = \"TextRank\"\n",
    "    else:\n",
    "        name = \"LSA\"\n",
    "    print(\"Summarizing with: \"+name)\n",
    "    summarizer = summarizer(stemmer)\n",
    "    summarizer.stop_words = get_stop_words(LANGUAGE)\n",
    "    for sentence in summarizer(parser.document, SENTENCES_COUNT):\n",
    "        print(sentence)\n",
    "        # Printing the number of the sentence in the original article\n",
    "        print(body_text[0:body_text.index(str(sentence))].count(\"\\n\")+1)\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
