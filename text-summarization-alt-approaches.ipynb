{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Alternative text summarization techniques\n",
    "\n",
    "Here we experiment with different Python Libraries and techniques to tackle our text summarization task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "import os, sys\n",
    "from IPython.display import display\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Loading one of the articles to work on\n",
    "filename = os.listdir(data_path+\"2016/1/\")[0]\n",
    "file = open(data_path+\"2016/1/\"+str(filename), \"r\")\n",
    "content = json.load(file)\n",
    "file.close()\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "body_raw = unicodedata.normalize(\"NFKD\",content[\"body\"]).encode(\"ascii\", \"ignore\")\n",
    "soup = BeautifulSoup(body_raw, \"html.parser\")\n",
    "# soup.text\n",
    "body_text = \"\\n\".join(tokenizer.tokenize(soup.get_text().replace(\"\\n\", \" \").replace(\"\\r\", \"\").replace(\"=\",\" \").replace(\"\\t\", \" \")))\n",
    "# body_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Gensim will calculate the most important sentences from the document, using TextRank and will return to us a summary\n",
    "# using only these sentences. It does not generate new sentences,\n",
    "# it just identifies and copies the most significant sentences.\n",
    "# More information at https://github.com/RaRe-Technologies/gensim\n",
    "from gensim.summarization import summarize, keywords\n",
    "\n",
    "# We can summarize the given article/document by defining the size of the summary \n",
    "# as a percentage of the size of the original one\n",
    "display(summarize(body_text, ratio=0.15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Or as an absolute word count\n",
    "display(summarize(body_text, word_count=50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# We can also extract the keywords from the document\n",
    "display(keywords(body_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Sumy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Sumy provides a variety of algorithms for text summarization, among which also TextRank.\n",
    "# More information about Sumy can be found at https://pypi.python.org/pypi/sumy\n",
    "\n",
    "from sumy.parsers.html import HtmlParser\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer\n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
    "from sumy.summarizers.text_rank import TextRankSummarizer\n",
    "from sumy.nlp.stemmers import Stemmer\n",
    "from sumy.utils import get_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Here we get summaries using TextRank, LexRank and LSA.\n",
    "# All three summaries produced are different, so it might be a good idea to use all three of them to find the \n",
    "# sentences on which they are overlapping, these sentences are more likely to contain valuable information.\n",
    "\n",
    "LANGUAGE = \"english\"\n",
    "SENTENCES_COUNT = 2\n",
    "parser = PlaintextParser.from_string(body_text, Tokenizer(LANGUAGE))\n",
    "# or for plain text files\n",
    "# parser = PlaintextParser.from_file(\"document.txt\", Tokenizer(LANGUAGE))\n",
    "stemmer = Stemmer(LANGUAGE)\n",
    "summarizers = [LexRankSummarizer, TextRankSummarizer, LsaSummarizer]\n",
    "for summarizer in summarizers:\n",
    "    if summarizers.index(summarizer) == 0:\n",
    "        name = \"LexRank\"\n",
    "    elif summarizers.index(summarizer) == 1:\n",
    "        name = \"TextRank\"\n",
    "    else:\n",
    "        name = \"LSA\"\n",
    "    print(\"Summarizing with: \"+name)\n",
    "    summarizer = summarizer(stemmer)\n",
    "    summarizer.stop_words = get_stop_words(LANGUAGE)\n",
    "    for sentence in summarizer(parser.document, SENTENCES_COUNT):\n",
    "        print(sentence)\n",
    "        # Printing the number of the sentence in the original article\n",
    "        print(body_text[0:body_text.index(str(sentence))].count(\"\\n\")+1)\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
