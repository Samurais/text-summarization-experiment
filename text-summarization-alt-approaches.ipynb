{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Alternative text summarization techniques\n",
    "\n",
    "Here we experiment with different Python Libraries and techniques to tackle our text summarization task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "import os, sys\n",
    "from IPython.display import display\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Define the path of the directory where the data is stored\n",
    "data_path = \"/home/gkastro/title-prediction-tensorflow/content-data/\"\n",
    "\n",
    "# Loading one of the articles to work on\n",
    "filename = os.listdir(data_path+\"2016/1/\")[0]\n",
    "file = open(data_path+\"2016/1/\"+str(filename), \"r\")\n",
    "content = json.load(file)\n",
    "file.close()\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "body_raw = unicodedata.normalize(\"NFKD\",content[\"body\"]).encode(\"ascii\", \"ignore\")\n",
    "soup = BeautifulSoup(body_raw, \"html.parser\")\n",
    "# soup.text\n",
    "body_text = \"\\n\".join(tokenizer.tokenize(soup.get_text().replace(\"\\n\", \" \").replace(\"\\r\", \"\").replace(\"=\",\" \").replace(\"\\t\", \" \")))\n",
    "# body_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Gensim will calculate the most important sentences from the document, using TextRank and will return to us a summary\n",
    "# using only these sentences. It does not generate new sentences,\n",
    "# it just identifies and copies the most significant sentences.\n",
    "# More information at https://github.com/RaRe-Technologies/gensim\n",
    "from gensim.summarization import summarize, keywords\n",
    "\n",
    "# We can summarize the given article/document by defining the size of the summary \n",
    "# as a percentage of the size of the original one\n",
    "display(summarize(body_text, ratio=0.15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Or as an absolute word count\n",
    "display(summarize(body_text, word_count=50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# We can also extract the keywords from the document\n",
    "print(keywords(body_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Sumy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Sumy provides a variety of algorithms for text summarization, among which also TextRank.\n",
    "# More information about Sumy can be found at https://pypi.python.org/pypi/sumy\n",
    "\n",
    "from sumy.parsers.html import HtmlParser\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer\n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
    "from sumy.summarizers.text_rank import TextRankSummarizer\n",
    "from sumy.nlp.stemmers import Stemmer\n",
    "from sumy.utils import get_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Here we get summaries using TextRank, LexRank and LSA.\n",
    "# All three summaries produced are different, so it might be a good idea to use all three of them to find the \n",
    "# sentences on which they are overlapping, these sentences are more likely to contain valuable information.\n",
    "\n",
    "LANGUAGE = \"english\"\n",
    "SENTENCES_COUNT = 1\n",
    "parser = PlaintextParser.from_string(body_text, Tokenizer(LANGUAGE))\n",
    "# or for plain text files\n",
    "# parser = PlaintextParser.from_file(\"document.txt\", Tokenizer(LANGUAGE))\n",
    "stemmer = Stemmer(LANGUAGE)\n",
    "summarizers = [LexRankSummarizer, TextRankSummarizer, LsaSummarizer]\n",
    "pop = []\n",
    "for summarizer in summarizers:\n",
    "    if summarizers.index(summarizer) == 0:\n",
    "        name = \"LexRank\"\n",
    "    elif summarizers.index(summarizer) == 1:\n",
    "        name = \"TextRank\"\n",
    "    else:\n",
    "        name = \"LSA\"\n",
    "    print(\"Summarizing with: \"+name)\n",
    "    summarizer = summarizer(stemmer)\n",
    "    summarizer.stop_words = get_stop_words(LANGUAGE)\n",
    "    for sentence in summarizer(parser.document, SENTENCES_COUNT):\n",
    "        print(sentence)\n",
    "        # Printing the number of the sentence in the original article\n",
    "        sen_num = (body_text[0:body_text.index(str(sentence))].count(\"\\n\")+1)\n",
    "        print(sen_num)\n",
    "        # Inserting the number of the sentence in the popularity list to see which sentences were picked by most models\n",
    "        # as important\n",
    "        pop.append(sen_num-1)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    " ### Rake\n",
    " Keyword - Keyphrase extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import RAKE\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "rake_object = RAKE.Rake(\"SmartStoplist.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "keywords = rake_object.run(soup.text)\n",
    "for key in keywords:\n",
    "    print(key)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Keyword Generator\n",
    "\n",
    "KG is a command line tool for extracting keywords from text, using it is very simple and doesn't require any code.\n",
    "\n",
    "We need the documents in .txt format and then run the appropriate command in order to retrieve the keywords.\n",
    "\n",
    "More info [here](https://github.com/jlonij/keyword-generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### De-referencing titles to articles\n",
    "\n",
    "Our Tensorflow model creates titles and stores them in a txt file, without any information about the article to which the title corresponds.\n",
    "The following lines of code search in all the articles to find the one corresponding to the title produced, giving back the UUID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "decode_path = \"/home/gkastro/decode/unseen-data/\"\n",
    "for filename in os.listdir(decode_path):\n",
    "    if filename.startswith(\"ref\"):\n",
    "        file = open(decode_path+filename, \"r\")\n",
    "        decode_data = file.read()\n",
    "        file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "decode_titles = \"\".join(decode_data.replace(\"output=\", \"\")).splitlines()\n",
    "decode_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_path = \"/home/gkastro/title-prediction-tensorflow/content-data/\"\n",
    "\n",
    "# Loading and searching the article titles\n",
    "year = 2017\n",
    "for month in range(1,4):\n",
    "    for filename in os.listdir(data_path+str(year)+\"/\"+str(month)):\n",
    "        file = open(data_path+str(year)+\"/\"+str(month)+\"/\"+str(filename), \"r\")\n",
    "        content = json.load(file)\n",
    "        file.close()\n",
    "        if content[\"title\"][\"title\"] == decode_titles[0]:\n",
    "            print(content[\"apiUrl\"][content[\"apiUrl\"].rfind(\"/\")+1:len(content[\"apiUrl\"])])\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
